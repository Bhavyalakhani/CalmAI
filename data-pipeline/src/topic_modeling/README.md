# Topic Modeling Pipeline

BERTopic-based topic modeling for the CalmAI platform — trains, validates, and serves three independent topic models (journals, conversations, and severity) for bias analysis, patient analytics, and real-time classification.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Module Structure](#module-structure)
- [Training Pipeline](#training-pipeline)
  - [Data Preparation](#data-preparation)
  - [Model Architecture](#model-architecture)
  - [Representation Models](#representation-models)
  - [Hyperparameter Tuning](#hyperparameter-tuning)
- [Inference](#inference)
- [Validation](#validation)
- [Bias Analysis](#bias-analysis)
- [Experiment Tracking](#experiment-tracking)
- [Model Registry](#model-registry)
- [Configuration](#configuration)
- [Integration Points](#integration-points)
- [Testing](#testing)

---

## Overview

CalmAI uses **BERTopic** for unsupervised topic discovery across three models:

1. **Journal model** (`bertopic_journals`) — classifies patient journal entries for per-patient analytics, bias detection, and real-time theme tagging in the therapist dashboard
2. **Conversation model** (`bertopic_conversations`) — classifies therapist-patient exchanges for conversation bias analysis and response quality monitoring
3. **Severity model** (`bertopic_severity`) — clusters conversations by emotional intensity and labels clusters as `crisis`, `severe`, `moderate`, or `mild` via Gemini LLM. Used by bias analysis and the therapist dashboard

All three models use the same embedding backbone (`sentence-transformers/all-MiniLM-L6-v2`, 384 dims) and share the same training/validation/serving architecture. Topic labels are generated by **Gemini LLM** via BERTopic's LangChain representation model, producing clinically descriptive labels (e.g. "Anxiety and daily worry" instead of "Topic 0"). The severity model uses a specialized prompt that instructs Gemini to assign one of the four severity levels based on cluster content.

All topic classification flows through the trained models — there are no keyword-based fallbacks. When a model is unavailable, pipeline bias tasks raise `RuntimeError`, and runtime components (backend, patient analytics) return `"unclassified"`. The severity model returns `"unknown"` when unavailable.

## Architecture

```
                ┌──────────────────────────────────┐
                │      Training (trainer.py)        │
                │  UMAP → HDBSCAN → BERTopic        │
                │  + Gemini LLM labels               │
                │  3 models: journals, conversations,│
                │            severity                │
                └──────────┬───────────────────────┘
                           │ safetensors
                           ▼
              ┌────────────────────────────┐
              │  Model Artifacts           │
              │  models/bertopic_{type}/   │
              │  └── latest/model          │
              └──────────┬─────────────────┘
                         │
          ┌──────────────┼──────────────────┐
          ▼              ▼                  ▼
  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
  │  Inference   │ │  Validation  │ │    Bias      │
  │ inference.py │ │ validation.py│ │bias_analysis │
  └──────┬───────┘ └──────┬───────┘ └──────┬───────┘
         │                │                │
         ▼                ▼                ▼
  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
  │ Patient      │ │  Quality     │ │  Topic Bias  │
  │ Analytics    │ │  Reports     │ │  Reports     │
  │ + Severity   │ │  (JSON)      │ │  (JSON+PNG)  │
  │ + Backend    │ │              │ │              │
  └──────────────┘ └──────────────┘ └──────────────┘

                ┌─────────────────────────────┐
                │     MLflow (mlruns/)         │
                │  Params, metrics, artifacts  │
                │  experiment_tracker.py       │
                └─────────────────────────────┘
```

## Module Structure

| File | Lines | Description |
|------|-------|-------------|
| `__init__.py` | 2 | Module docstring |
| `config.py` | 163 | `TopicModelConfig` dataclass, `HyperparameterSpace`, `SeverityHyperparameterSpace`, Gemini prompt templates (including `SEVERITY_LABEL_PROMPT`), path helpers |
| `trainer.py` | 615 | `TopicModelTrainer` — builds UMAP/HDBSCAN/BERTopic pipeline, trains models (journals, conversations, severity), hyperparameter tuning, model saving |
| `inference.py` | 314 | `TopicModelInference` — loads saved models, predict topics, severity prediction (`predict_severity`, `predict_severity_series`), labels/keywords/distributions |
| `validation.py` | 263 | `TopicModelValidator` — quality metrics (diversity, coherence, outlier ratio, Gini), pass/fail checks |
| `bias_analysis.py` | 575 | `TopicBiasAnalyzer` — topic distribution balance, severity analysis (BERTopic-based), underrepresentation, patient coverage, temporal patterns |
| `experiment_tracker.py` | 107 | `MLflowTracker` — experiment logging, run management, model registry tagging. Uses `Path.as_uri()` for cross-platform MLflow URI support |

## Training Pipeline

### Data Preparation

Documents are prepared from processed parquet files:

```python
from topic_modeling.trainer import TopicModelTrainer

trainer = TopicModelTrainer(model_type="journals")

# journal docs: "[YYYY-MM-DD] content"
docs = trainer.prepare_journal_docs(journals_df)

# conversation docs: "User concern: {context}\n\nCounselor response: {response}"
docs = trainer.prepare_conversation_docs(conversations_df)
```

### Model Architecture

Each BERTopic model is constructed from:

1. **Embedding model** — `sentence-transformers/all-MiniLM-L6-v2` (384 dims, shared with the embedding pipeline)
2. **UMAP** — dimensionality reduction (cosine metric, configurable neighbors/components)
3. **HDBSCAN** — density-based clustering (configurable min_cluster_size/min_samples)
4. **CountVectorizer** — c-TF-IDF for topic keyword extraction (ngram_range=(1,2))
5. **Representation models** — multi-aspect: KeyBERTInspired + Gemini LLM + MaximalMarginalRelevance

### Representation Models

BERTopic uses three complementary representation models:

| Aspect | Model | Purpose |
|--------|-------|---------|
| `KeyBERT` | `KeyBERTInspired` | Embedding-based keyword refinement |
| `LLM` | Gemini via LangChain | Clinically descriptive topic labels (6 words max) |
| `MMR` | `MaximalMarginalRelevance` | Diversity-optimized keywords (diversity=0.3) |

The **LLM aspect** uses custom prompts tailored to the mental health domain. BERTopic sends `[DOCUMENTS]` and `[KEYWORDS]` placeholders that Gemini fills with a short label.

### Hyperparameter Tuning

The `tune()` method performs grid search over configurable hyperparameter spaces:

```python
# journal hyperparameter space
HyperparameterSpace:
  umap_n_neighbors: [10, 15, 20]
  umap_n_components: [3, 5]
  hdbscan_min_cluster_size: [5, 8, 10]
  hdbscan_min_samples: [3, 5]
  top_n_words: [5, 10]

# conversation hyperparameter space (larger clusters for longer docs)
ConversationHyperparameterSpace:
  umap_n_neighbors: [10, 15, 20]
  umap_n_components: [5, 10]
  hdbscan_min_cluster_size: [10, 15, 20]
  hdbscan_min_samples: [5, 10]
  top_n_words: [10, 15]

# severity hyperparameter space (broader clusters for 4 severity levels)
SeverityHyperparameterSpace:
  umap_n_neighbors: [10, 15, 20]
  umap_n_components: [3, 5]
  hdbscan_min_cluster_size: [15, 20, 30]
  hdbscan_min_samples: [5, 10]
  top_n_words: [10, 15]
```

Each configuration is trained, validated, and scored with a **composite score**:

```
composite_score = 0.3 × topic_diversity + 0.3 × (1 - outlier_ratio) + 0.2 × (1 - size_gini) + 0.2 × label_quality
```

The best model (highest composite score) is saved and tagged as `best` in MLflow.

## Inference

```python
from topic_modeling.inference import TopicModelInference

inf = TopicModelInference(model_type="journals")
if inf.load():
    # single prediction
    result = inf.predict_single("I feel anxious and worried today")
    # result: {"topic_id": 3, "label": "Anxiety and daily worry", "keywords": [...], "probability": 0.87}

    # batch prediction
    topics, probs = inf.predict(["doc1", "doc2", "doc3"])

    # full distribution
    distribution = inf.get_topic_distribution(topics)
    # [{"topic_id": 0, "label": "...", "keywords": [...], "count": 5, "percentage": 25.0}, ...]
```

### Severity Prediction

```python
from topic_modeling.inference import TopicModelInference

severity_inf = TopicModelInference(model_type="severity")
if severity_inf.load():
    # batch severity classification
    severities = severity_inf.predict_severity(["I feel like giving up", "Had a good day today"])
    # ["crisis", "mild"]

    # pandas series variant
    severity_series = severity_inf.predict_severity_series(df["embedding_text"])
    # pd.Series(["severe", "moderate", "mild", ...])
```

Severity labels are derived from Gemini-assigned topic labels. The `_topic_to_severity()` method checks if a label contains one of `{"crisis", "severe", "moderate", "mild"}`. Topic -1 (outlier) and labels without a matching keyword map to `"unknown"`.

A convenience singleton wrapper (`src/bias_detection/severity.py`) provides `classify_severity()`, `classify_severity_batch()`, and `classify_severity_series()` for use outside the topic modeling module.

When the model is not available:
- **Pipeline bias tasks** (`journal_bias.py`, `conversation_bias.py`): raise `RuntimeError` — the model must be trained before bias analysis runs
- **Patient analytics** (`patient_analytics.py`): returns `"unclassified"` with `model_version: "unavailable"`
- **Backend** (`journals.py`): returns `["unclassified"]` themes

## Validation

The `TopicModelValidator` checks model quality after training:

| Metric | Description | Default Threshold |
|--------|-------------|-------------------|
| `topic_count` | Number of discovered topics | 3–50 |
| `outlier_ratio` | Fraction of docs assigned to topic -1 | < 0.4 |
| `topic_diversity` | Uniqueness of keywords across topics (0–1) | > 0.5 |
| `avg_topic_size` | Mean docs per topic | > 5 |
| `label_uniqueness` | Fraction of unique LLM-generated labels | > 0.8 |
| `size_gini` | Gini coefficient of topic sizes (0=equal, 1=skewed) | < 0.6 |
| `composite_score` | Weighted combination of above metrics | (logged to MLflow) |

Validation reports are saved as JSON to `reports/model/`.

## Bias Analysis

`TopicBiasAnalyzer` uses the trained model to detect bias in the dataset:

- **Topic distribution balance** — flags topics with < 3% representation
- **Patient topic coverage** — checks if patients write about diverse topics
- **Temporal patterns** — topic shifts over time
- **Outlier analysis** — entries that don't fit any topic
- **Underrepresentation detection** — topics below configurable threshold

Results feed into the main bias reports (`journal_bias_report.json`, `conversation_bias_report.json`) and visualizations.

## Experiment Tracking

All training runs are logged to **MLflow** (local file-backed at `data-pipeline/mlruns/`):

- **Experiments**: `journal_topic_model`, `conversation_topic_model`, `severity_topic_model`
- **Logged per run**: hyperparameters, validation metrics, composite score, model artifacts
- **Best model tagging**: `tag_best_model(run_id)` marks the best run for each model type
- **Querying**: `get_best_run()` retrieves the highest-scoring run

MLflow UI: `cd data-pipeline && mlflow ui` → http://localhost:5000

## Model Registry

Models are saved using BERTopic's safetensors serialization:

```
data-pipeline/models/
├── bertopic_journals/
│   └── latest/
│       └── model/          # safetensors artifacts
├── bertopic_conversations/
│   └── latest/
│       └── model/          # safetensors artifacts
└── bertopic_severity/
    └── latest/
        └── model/          # safetensors artifacts
```

Model loading is a single call: `TopicModelInference(model_type="journals").load()`.

## Configuration

All configuration flows through `TopicModelConfig`:

```python
from topic_modeling.config import TopicModelConfig

config = TopicModelConfig(
    model_type="journals",
    umap_n_neighbors=15,
    umap_n_components=5,
    hdbscan_min_cluster_size=10,
    hdbscan_min_samples=5,
    top_n_words=10,
    use_gemini_labels=True,
)
```

Environment variables (loaded via `configs/config.py`):
- `GEMINI_API_KEY` — required for LLM labeling
- `GEMINI_MODEL` — model name (default: `gemini-2.5-flash`)
- `EMBEDDING_MODEL` — sentence-transformers model name

## Integration Points

| Consumer | Module | Usage |
|----------|--------|-------|
| **DAG 1** (batch pipeline) | `trainer.py` | `train_journal_model`, `train_conversation_model`, and `train_severity_model` tasks |
| **DAG 1** (bias analysis) | `inference.py` | `journal_bias.py` and `conversation_bias.py` classify topics; severity model used by `bias_analysis.py` |
| **DAG 1** (patient analytics) | `inference.py` | `compute_patient_analytics` task uses journal model for topic distribution |
| **DAG 1** (MongoDB storage) | `inference.py` | `store_to_mongodb` task calls `classify_and_update_conversations()` to assign topic + severity to conversations |
| **DAG 2** (incoming journals) | `inference.py` | `update_analytics` task reclassifies patient topics |
| **Backend** (journals router) | `inference.py` | Real-time theme classification for new journal entries |
| **Severity wrapper** | `severity.py` | Singleton wrapper around `TopicModelInference(model_type="severity")` for `mongodb_client`, `conversation_bias`, `bias_analysis` |
| **Validation** (DAG 1) | `validation.py` | Quality checks after training |

### DAG 1 task dependency

```
embed_conversations ──┐
embed_journals ───────┤
                      ▼
              embedding_complete
                      │
        ┌─────────────┼──────────────────┐
        ▼             ▼                  ▼
train_journal_model  train_conversation_model  train_severity_model
        │             │                  │
        └─────────────┼──────────────────┘
                      ▼
              training_complete
                      │
           ┌──────────┼──────────┐
           ▼                     ▼
    bias_conversations    bias_journals
           │                     │
           └──────────┬──────────┘
                      ▼
                bias_complete
                      ▼
          compute_patient_analytics
                      ▼
              store_to_mongodb
                      ▼
               success_email
                      ▼
                     end
```

## Testing

Tests are in `data-pipeline/tests/`:

| File | Tests | Coverage |
|------|-------|----------|
| `test_topic_modeling.py` | ~35 | Trainer, inference, validator, config, experiment tracker |
| `test_topic_bias.py` | ~15 | TopicBiasAnalyzer — distributions, coverage, temporal |

Run tests:

```bash
cd data-pipeline
pytest tests/test_topic_modeling.py tests/test_topic_bias.py -v
```

All external dependencies (BERTopic, UMAP, HDBSCAN, MLflow, Gemini) are mocked in tests. The test suite runs without GPU, model files, or API keys.

> **Note**: UMAP and HDBSCAN are C-extension packages that may not install on all Python versions (e.g. Python 3.14). They are lazy-imported at runtime and available in the Airflow Docker container. IDE import errors are suppressed with `# type: ignore[import-untyped]`.
