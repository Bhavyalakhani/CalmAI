# dvc pipeline stages mirroring the airflow dag
# tracks data artifacts from acquisition through embedding
# run with: dvc repro

stages:
  download_conversations:
    wdir: data-pipeline
    cmd: python src/acquisition/data_downloader.py
    deps:
      - src/acquisition/data_downloader.py
      - configs/config.py
    outs:
      - data/raw/conversations/mental_health_conversations.parquet
      - data/raw/conversations/counsel_chat.parquet

  generate_journals:
    wdir: data-pipeline
    cmd: python src/acquisition/generate_journals.py run
    deps:
      - src/acquisition/generate_journals.py
      - configs/config.py
      - configs/patient_profiles.yaml
    outs:
      - data/raw/journals/synthetic_journals.parquet

  preprocess_conversations:
    wdir: data-pipeline
    cmd: python -c "from src.preprocessing.conversation_preprocessor import ConversationPreprocessor; ConversationPreprocessor().run(skip_existing=False)"
    deps:
      - src/preprocessing/conversation_preprocessor.py
      - src/preprocessing/base_preprocessor.py
      - data/raw/conversations/mental_health_conversations.parquet
      - data/raw/conversations/counsel_chat.parquet
    outs:
      - data/processed/conversations/processed_conversations.parquet

  preprocess_journals:
    wdir: data-pipeline
    cmd: python -c "from src.preprocessing.journal_preprocessor import JournalPreprocessor; JournalPreprocessor().run(skip_existing=False)"
    deps:
      - src/preprocessing/journal_preprocessor.py
      - src/preprocessing/base_preprocessor.py
      - data/raw/journals/synthetic_journals.parquet
    outs:
      - data/processed/journals/processed_journals.parquet

  validate:
    wdir: data-pipeline
    cmd: python -c "from src.validation.schema_validator import SchemaValidator; SchemaValidator().run(skip_existing=False)"
    deps:
      - src/validation/schema_validator.py
      - data/processed/conversations/processed_conversations.parquet
      - data/processed/journals/processed_journals.parquet
    outs:
      - reports/schema/conversations_schema_report.json
      - reports/schema/journals_schema_report.json

  bias_conversations:
    wdir: data-pipeline
    cmd: python -c "from src.bias_detection.conversation_bias import ConversationBiasAnalyzer; ConversationBiasAnalyzer().run(skip_existing=False)"
    deps:
      - src/bias_detection/conversation_bias.py
      - src/bias_detection/slicer.py
      - data/processed/conversations/processed_conversations.parquet
      - models/bertopic_conversations/
      - models/bertopic_severity/
    outs:
      - reports/bias/conversation_bias_report.json
      - reports/bias/topic_distribution.png
      - reports/bias/severity_distribution.png
      - reports/bias/response_length_by_topic.png

  bias_journals:
    wdir: data-pipeline
    cmd: python -c "from src.bias_detection.journal_bias import JournalBiasAnalyzer; JournalBiasAnalyzer().run(skip_existing=False)"
    deps:
      - src/bias_detection/journal_bias.py
      - src/bias_detection/slicer.py
      - data/processed/journals/processed_journals.parquet
      - models/bertopic_journals/
    outs:
      - reports/bias/journal_bias_report.json
      - reports/bias/journal_theme_distribution.png
      - reports/bias/journal_entries_by_day.png
      - reports/bias/journal_entries_by_month.png

  embed_conversations:
    wdir: data-pipeline
    cmd: python -c "from src.embedding.embedder import embed_conversations; embed_conversations(force=True)"
    deps:
      - src/embedding/embedder.py
      - data/processed/conversations/processed_conversations.parquet
    outs:
      - data/processed/conversations/embedded_conversations.parquet

  embed_journals:
    wdir: data-pipeline
    cmd: python -c "from src.embedding.embedder import embed_journals; embed_journals(force=True)"
    deps:
      - src/embedding/embedder.py
      - data/processed/journals/processed_journals.parquet
    outs:
      - data/processed/journals/embedded_journals.parquet

  train_journal_model:
    wdir: data-pipeline
    cmd: >-
      python -c "
      import pandas as pd, numpy as np;
      from src.topic_modeling.trainer import TopicModelTrainer;
      from src.topic_modeling.config import TopicModelConfig;
      from src.topic_modeling.validation import TopicModelValidator;
      cfg = TopicModelConfig(model_type='journals');
      trainer = TopicModelTrainer(cfg);
      df = pd.read_parquet('data/processed/journals/embedded_journals.parquet');
      docs, ts = trainer.prepare_journal_docs(df);
      emb = np.array(df['embedding'].tolist()) if 'embedding' in df.columns else None;
      result = trainer.train(docs, embeddings=emb, timestamps=ts);
      trainer.save_model();
      v = TopicModelValidator(); r = v.validate(result); v.save_report(r)
      "
    deps:
      - src/topic_modeling/trainer.py
      - src/topic_modeling/config.py
      - src/topic_modeling/validation.py
      - data/processed/journals/embedded_journals.parquet
    outs:
      - models/bertopic_journals/:
          persist: true

  train_conversation_model:
    wdir: data-pipeline
    cmd: >-
      python -c "
      import pandas as pd, numpy as np;
      from src.topic_modeling.trainer import TopicModelTrainer;
      from src.topic_modeling.config import TopicModelConfig;
      from src.topic_modeling.validation import TopicModelValidator;
      cfg = TopicModelConfig(model_type='conversations');
      trainer = TopicModelTrainer(cfg);
      df = pd.read_parquet('data/processed/conversations/embedded_conversations.parquet');
      docs, _ = trainer.prepare_conversation_docs(df);
      emb = np.array(df['embedding'].tolist()) if 'embedding' in df.columns else None;
      result = trainer.train(docs, embeddings=emb);
      trainer.save_model();
      v = TopicModelValidator(); r = v.validate(result); v.save_report(r)
      "
    deps:
      - src/topic_modeling/trainer.py
      - src/topic_modeling/config.py
      - src/topic_modeling/validation.py
      - data/processed/conversations/embedded_conversations.parquet
    outs:
      - models/bertopic_conversations/:
          persist: true

  train_severity_model:
    wdir: data-pipeline
    cmd: >-
      python -c "
      import pandas as pd, numpy as np;
      from src.topic_modeling.trainer import TopicModelTrainer;
      from src.topic_modeling.config import TopicModelConfig;
      from src.topic_modeling.validation import TopicModelValidator;
      cfg = TopicModelConfig(model_type='severity');
      trainer = TopicModelTrainer(cfg);
      df = pd.read_parquet('data/processed/conversations/embedded_conversations.parquet');
      docs, _ = trainer.prepare_conversation_docs(df);
      emb = np.array(df['embedding'].tolist()) if 'embedding' in df.columns else None;
      result = trainer.train(docs, embeddings=emb);
      trainer.save_model();
      v = TopicModelValidator(); r = v.validate(result); v.save_report(r)
      "
    deps:
      - src/topic_modeling/trainer.py
      - src/topic_modeling/config.py
      - src/topic_modeling/validation.py
      - data/processed/conversations/embedded_conversations.parquet
    outs:
      - models/bertopic_severity/:
          persist: true
